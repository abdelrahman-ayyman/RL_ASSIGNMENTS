{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1a2c00a88034fc68d9ccb8a2f6acb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11a70b73c0e141aba83e47c4b296e707",
              "IPY_MODEL_92ca74a82cd143168c463511dc77e0ed",
              "IPY_MODEL_fadb9bb15ab34a27bc91d4214b925569"
            ],
            "layout": "IPY_MODEL_a9fc2ae9be0144c6b4a2648a9d96c850"
          }
        },
        "11a70b73c0e141aba83e47c4b296e707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e717d73777934a0b917888828eeea39d",
            "placeholder": "​",
            "style": "IPY_MODEL_ad8814820c6b4839859f703a06a667dc",
            "value": "100%"
          }
        },
        "92ca74a82cd143168c463511dc77e0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d8677ee925b4d5a973226e12e224797",
            "max": 203125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6291b5ea04d14968afef816cac8e32c1",
            "value": 203125
          }
        },
        "fadb9bb15ab34a27bc91d4214b925569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67e2bb99e5a546b6a62f5b85c631c513",
            "placeholder": "​",
            "style": "IPY_MODEL_20812befa52e4038a669c58e9fbe16fd",
            "value": " 203125/203125 [00:59&lt;00:00, 3824.35it/s]"
          }
        },
        "a9fc2ae9be0144c6b4a2648a9d96c850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e717d73777934a0b917888828eeea39d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad8814820c6b4839859f703a06a667dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d8677ee925b4d5a973226e12e224797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6291b5ea04d14968afef816cac8e32c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67e2bb99e5a546b6a62f5b85c631c513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20812befa52e4038a669c58e9fbe16fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNQBA4EYN9lf",
        "outputId": "13e4a722-4ce1-476a-bffc-c3099b040788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium version: 1.2.1\n",
            "Pygame version: 2.6.1\n",
            "Numpy version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import pygame\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from collections.abc import Mapping\n",
        "\n",
        "# Helper function to flatten the complex observation space for state mapping\n",
        "def flatten_obs(obs):\n",
        "    \"\"\"Flattens the MultiDiscrete observation into a single tuple.\"\"\"\n",
        "    return tuple(obs.flatten())\n",
        "\n",
        "print(f\"Gymnasium version: {gym.__version__}\")\n",
        "print(f\"Pygame version: {pygame.version.ver}\")\n",
        "print(f\"Numpy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridMazeEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Stochastic Grid Maze Environment based on Gymnasium.\n",
        "\n",
        "    State: [agent_x, agent_y, goal_x, goal_y, bad1_x, bad1_y, bad2_x, bad2_y]\n",
        "    Actions: 0: Right, 1: Up, 2: Left, 3: Down\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, size=5, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.window_size = 512  # Pygame window size\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "        # Observation Space: [ax, ay, gx, gy, b1x, b1y, b2x, b2y]\n",
        "        # Each coordinate is from 0 to size-1\n",
        "        self.observation_space = spaces.MultiDiscrete(np.array([size] * 8))\n",
        "\n",
        "        # Action Space: 4 discrete actions\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Action to (x, y) change mapping\n",
        "        # 0: Right, 1: Up, 2: Left, 3: Down\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),  # Right\n",
        "            1: np.array([0, -1]), # Up\n",
        "            2: np.array([-1, 0]), # Left\n",
        "            3: np.array([0, 1]),  # Down\n",
        "        }\n",
        "\n",
        "        # Perpendicular directions for stochasticity\n",
        "        self._perpendicular_dirs = {\n",
        "            0: [np.array([0, -1]), np.array([0, 1])],  # Right -> Up/Down\n",
        "            1: [np.array([-1, 0]), np.array([1, 0])],  # Up -> Left/Right\n",
        "            2: [np.array([0, -1]), np.array([0, 1])],  # Left -> Up/Down\n",
        "            3: [np.array([-1, 0]), np.array([1, 0])],  # Down -> Left/Right\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._pygame_init()\n",
        "\n",
        "    def _pygame_init(self):\n",
        "        \"\"\"Initialize PyGame for rendering.\"\"\"\n",
        "        pygame.init()\n",
        "        pygame.display.init()\n",
        "        self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self._font = pygame.font.Font(None, 36)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"Get the current observation from the state.\"\"\"\n",
        "        return np.array([\n",
        "            self._agent_location[0], self._agent_location[1],\n",
        "            self._goal_location[0], self._goal_location[1],\n",
        "            self._bad1_location[0], self._bad1_location[1],\n",
        "            self._bad2_location[0], self._bad2_location[1]\n",
        "        ])\n",
        "\n",
        "    def _get_info(self):\n",
        "        \"\"\"Get auxiliary info.\"\"\"\n",
        "        return {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Choose 4 unique locations for agent, goal, and bad cells\n",
        "        locations = self.np_random.choice(\n",
        "            self.size * self.size, 4, replace=False\n",
        "        )\n",
        "        coords = [np.array([loc % self.size, loc // self.size]) for loc in locations]\n",
        "\n",
        "        self._agent_location = coords[0]\n",
        "        self._goal_location = coords[1]\n",
        "        self._bad1_location = coords[2]\n",
        "        self._bad2_location = coords[3]\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _design_reward(self, terminated):\n",
        "        \"\"\"Implements the reward function as required.\"\"\"\n",
        "        if terminated:\n",
        "            if np.array_equal(self._agent_location, self._goal_location):\n",
        "                return 1.0  # High positive reward for reaching the goal\n",
        "            else:\n",
        "                return -1.0 # High negative reward for hitting a bad cell\n",
        "        else:\n",
        "            return -0.01 # Small negative penalty for each step\n",
        "\n",
        "    def step(self, action):\n",
        "        # Stochastic movement logic\n",
        "        p = self.np_random.random()\n",
        "\n",
        "        if p < 0.70:\n",
        "            # 70% chance of intended direction\n",
        "            direction = self._action_to_direction[action]\n",
        "        elif p < 0.85:\n",
        "            # 15% chance of perpendicular 1\n",
        "            direction = self._perpendicular_dirs[action][0]\n",
        "        else:\n",
        "            # 15% chance of perpendicular 2\n",
        "            direction = self._perpendicular_dirs[action][1]\n",
        "\n",
        "        # Apply movement and clip to stay within grid boundaries (0 to size-1)\n",
        "        self._agent_location = np.clip(\n",
        "            self._agent_location + direction, 0, self.size - 1\n",
        "        )\n",
        "\n",
        "        # Check for termination\n",
        "        terminated = (\n",
        "            np.array_equal(self._agent_location, self._goal_location) or\n",
        "            np.array_equal(self._agent_location, self._bad1_location) or\n",
        "            np.array_equal(self._agent_location, self._bad2_location)\n",
        "        )\n",
        "\n",
        "        # Get reward\n",
        "        reward = self._design_reward(terminated)\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, reward, terminated, False, info # False for truncated\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "        elif self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            self._pygame_init()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_size = self.window_size / self.size\n",
        "\n",
        "        # Draw Goal (Green)\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (0, 255, 0),\n",
        "            pygame.Rect(\n",
        "                pix_size * self._goal_location[0],\n",
        "                pix_size * self._goal_location[1],\n",
        "                pix_size,\n",
        "                pix_size,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Draw Bad Cells (Red)\n",
        "        for bad_loc in [self._bad1_location, self._bad2_location]:\n",
        "            pygame.draw.rect(\n",
        "                canvas,\n",
        "                (255, 0, 0),\n",
        "                pygame.Rect(\n",
        "                    pix_size * bad_loc[0],\n",
        "                    pix_size * bad_loc[1],\n",
        "                    pix_size,\n",
        "                    pix_size,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        # Draw Agent (Blue)\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            (0, 0, 255),\n",
        "            (self._agent_location + 0.5) * pix_size,\n",
        "            pix_size / 3,\n",
        "        )\n",
        "\n",
        "        # Draw gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                (0, 0, 0),\n",
        "                (0, pix_size * x),\n",
        "                (self.window_size, pix_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                (0, 0, 0),\n",
        "                (pix_size * x, 0),\n",
        "                (pix_size * x, self.window_size),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.window:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.window = None"
      ],
      "metadata": {
        "id": "lpGEUOYVqUA9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StateMapper:\n",
        "    \"\"\"\n",
        "    Handles mapping the 8-tuple state to a unique integer index and back.\n",
        "    Implements the optimization for Q2 by treating bad cells as an unordered set.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.n_pos = size * size  # 25\n",
        "\n",
        "        # Calculate size of an unordered pair of bad cells (combinations w/ replacement)\n",
        "        # N*(N+1) / 2 = 25*(26)/2 = 325\n",
        "        self.n_bad_pairs = (self.n_pos * (self.n_pos + 1)) // 2\n",
        "\n",
        "        self.n_states = self.n_pos * self.n_pos * self.n_bad_pairs\n",
        "\n",
        "        # Pre-compute mapping for unordered pairs for speed\n",
        "        self._bad_pair_to_idx = {}\n",
        "        self._idx_to_bad_pair = [None] * self.n_bad_pairs\n",
        "        idx = 0\n",
        "        for b1 in range(self.n_pos):\n",
        "            for b2 in range(b1, self.n_pos): # b2 >= b1\n",
        "                self._bad_pair_to_idx[(b1, b2)] = idx\n",
        "                self._idx_to_bad_pair[idx] = (b1, b2)\n",
        "                idx += 1\n",
        "\n",
        "    def _map_bad_pair(self, b1_pos, b2_pos):\n",
        "        \"\"\"Maps an unordered (b1, b2) pair to a unique index 0-324.\"\"\"\n",
        "        if b1_pos > b2_pos:\n",
        "            b1_pos, b2_pos = b2_pos, b1_pos\n",
        "        return self._bad_pair_to_idx[(b1_pos, b2_pos)]\n",
        "\n",
        "    def _unmap_bad_pair(self, idx):\n",
        "        \"\"\"Maps an index 0-324 back to a (b1, b2) pair.\"\"\"\n",
        "        return self._idx_to_bad_pair[idx]\n",
        "\n",
        "    def state_to_index(self, obs):\n",
        "        \"\"\"Converts an 8-tuple observation [ax,ay,gx,gy,b1x,b1y,b2x,b2y] to an index.\"\"\"\n",
        "        ax, ay, gx, gy, b1x, b1y, b2x, b2y = obs\n",
        "\n",
        "        a_pos = ax * self.size + ay\n",
        "        g_pos = gx * self.size + gy\n",
        "        b1_pos = b1x * self.size + b1y\n",
        "        b2_pos = b2x * self.size + b2y\n",
        "\n",
        "        b_pair_idx = self._map_bad_pair(b1_pos, b2_pos)\n",
        "\n",
        "        # Final index calculation\n",
        "        index = (a_pos * self.n_pos * self.n_bad_pairs) + \\\n",
        "                (g_pos * self.n_bad_pairs) + \\\n",
        "                b_pair_idx\n",
        "        return index\n",
        "\n",
        "    def index_to_state(self, index):\n",
        "        \"\"\"Converts a unique index back to an 8-tuple state.\"\"\"\n",
        "        b_pair_idx = index % self.n_bad_pairs\n",
        "        index //= self.n_bad_pairs\n",
        "\n",
        "        g_pos = index % self.n_pos\n",
        "        index //= self.n_pos\n",
        "\n",
        "        a_pos = index\n",
        "\n",
        "        b1_pos, b2_pos = self._unmap_bad_pair(b_pair_idx)\n",
        "\n",
        "        return np.array([\n",
        "            a_pos // self.size, a_pos % self.size,  # ax, ay\n",
        "            g_pos // self.size, g_pos % self.size,  # gx, gy\n",
        "            b1_pos // self.size, b1_pos % self.size, # b1x, b1y\n",
        "            b2_pos // self.size, b2_pos % self.size  # b2x, b2y\n",
        "        ])\n",
        "\n",
        "# --- Test the mapper ---\n",
        "mapper = StateMapper(size=5)\n",
        "print(f\"Grid size: {mapper.size}x{mapper.size}\")\n",
        "print(f\"Total positions (N): {mapper.n_pos}\")\n",
        "print(f\"Unordered bad pairs: {mapper.n_bad_pairs}\")\n",
        "print(f\"Optimized State Space Size |S|: {mapper.n_states}\")\n",
        "\n",
        "# Test case: Bad1=(1,2), Bad2=(3,4)\n",
        "obs1 = np.array([0,0, 4,4, 1,2, 3,4])\n",
        "# Test case: Bad1=(3,4), Bad2=(1,2)\n",
        "obs2 = np.array([0,0, 4,4, 3,4, 1,2])\n",
        "\n",
        "idx1 = mapper.state_to_index(obs1)\n",
        "idx2 = mapper.state_to_index(obs2)\n",
        "\n",
        "print(f\"\\nIndex for obs1: {idx1}\")\n",
        "print(f\"Index for obs2: {idx2}\")\n",
        "assert idx1 == idx2\n",
        "print(\"Test PASSED: Symmetrical states map to the same index.\")\n",
        "\n",
        "restored_obs = mapper.index_to_state(idx1)\n",
        "print(f\"Restored obs from index: {restored_obs}\")\n",
        "# Note: The restored obs may have b1/b2 swapped, which is correct."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26MH2zw5qaMy",
        "outputId": "936866a4-6485-496d-c26f-d097a79794e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid size: 5x5\n",
            "Total positions (N): 25\n",
            "Unordered bad pairs: 325\n",
            "Optimized State Space Size |S|: 203125\n",
            "\n",
            "Index for obs1: 7966\n",
            "Index for obs2: 7966\n",
            "Test PASSED: Symmetrical states map to the same index.\n",
            "Restored obs from index: [0 0 4 4 1 2 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transition_model(mapper):\n",
        "    \"\"\"\n",
        "    Builds the P(s'|s,a) and R(s,a,s') model for the entire MDP.\n",
        "\n",
        "    Returns:\n",
        "    model[s_idx][a] = [ (prob, next_s_idx, reward), ... ]\n",
        "    \"\"\"\n",
        "    n_states = mapper.n_states\n",
        "    n_actions = 4\n",
        "    size = mapper.size\n",
        "\n",
        "    # Action map\n",
        "    _action_to_direction = {\n",
        "        0: np.array([1, 0]), 1: np.array([0, -1]),\n",
        "        2: np.array([-1, 0]), 3: np.array([0, 1])\n",
        "    }\n",
        "    _perpendicular_dirs = {\n",
        "        0: [np.array([0, -1]), np.array([0, 1])],\n",
        "        1: [np.array([-1, 0]), np.array([1, 0])],\n",
        "        2: [np.array([0, -1]), np.array([0, 1])],\n",
        "        3: [np.array([-1, 0]), np.array([1, 0])]\n",
        "    }\n",
        "\n",
        "    # model[s_idx][action] = list of (prob, next_s_idx, reward)\n",
        "    model = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n",
        "\n",
        "    print(f\"Building transition model for {n_states} states...\")\n",
        "\n",
        "    for s_idx in tqdm(range(n_states)):\n",
        "        state = mapper.index_to_state(s_idx)\n",
        "        agent_pos = state[0:2]\n",
        "        goal_pos = state[2:4]\n",
        "        bad1_pos = state[4:6]\n",
        "        bad2_pos = state[6:8]\n",
        "\n",
        "        # Check if s_idx is a terminal state\n",
        "        is_terminal = (\n",
        "            np.array_equal(agent_pos, goal_pos) or\n",
        "            np.array_equal(agent_pos, bad1_pos) or\n",
        "            np.array_equal(agent_pos, bad2_pos)\n",
        "        )\n",
        "\n",
        "        if is_terminal:\n",
        "            # In a terminal state, all actions lead to self-loop w/ 0 reward\n",
        "            for a in range(n_actions):\n",
        "                model[s_idx][a].append((1.0, s_idx, 0.0))\n",
        "            continue\n",
        "\n",
        "        # If not terminal, calculate transitions\n",
        "        for a in range(n_actions):\n",
        "            outcomes = [\n",
        "                (0.70, _action_to_direction[a]),\n",
        "                (0.15, _perpendicular_dirs[a][0]),\n",
        "                (0.15, _perpendicular_dirs[a][1])\n",
        "            ]\n",
        "\n",
        "            for prob, direction in outcomes:\n",
        "                # Get next agent position\n",
        "                next_agent_pos = np.clip(agent_pos + direction, 0, size - 1)\n",
        "\n",
        "                # Check for termination\n",
        "                terminated = (\n",
        "                    np.array_equal(next_agent_pos, goal_pos) or\n",
        "                    np.array_equal(next_agent_pos, bad1_pos) or\n",
        "                    np.array_equal(next_agent_pos, bad2_pos)\n",
        "                )\n",
        "\n",
        "                # Calculate reward\n",
        "                if terminated:\n",
        "                    reward = 1.0 if np.array_equal(next_agent_pos, goal_pos) else -1.0\n",
        "                else:\n",
        "                    reward = -0.01 # Step penalty\n",
        "\n",
        "                # Construct next state\n",
        "                next_state_obs = np.array([\n",
        "                    next_agent_pos[0], next_agent_pos[1],\n",
        "                    goal_pos[0], goal_pos[1],\n",
        "                    bad1_pos[0], bad1_pos[1],\n",
        "                    bad2_pos[0], bad2_pos[1]\n",
        "                ])\n",
        "                next_s_idx = mapper.state_to_index(next_state_obs)\n",
        "\n",
        "                # Store (prob, next_state, reward)\n",
        "                model[s_idx][a].append((prob, next_s_idx, reward))\n",
        "\n",
        "    print(\"Model build complete.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def policy_iteration(model, mapper, gamma=0.99, theta=1e-6):\n",
        "    \"\"\"\n",
        "    Performs Policy Iteration.\n",
        "\n",
        "    Args:\n",
        "    - model: The transition model P(s'|s,a)\n",
        "    - mapper: The StateMapper object\n",
        "    - gamma: Discount factor\n",
        "    - theta: Convergence threshold for Policy Evaluation\n",
        "    \"\"\"\n",
        "    n_states = mapper.n_states\n",
        "    n_actions = 4\n",
        "\n",
        "    # 1. Initialize V(s) and pi(s)\n",
        "    V = np.zeros(n_states)\n",
        "    policy = np.zeros(n_states, dtype=int) # Default to action 0 (Right)\n",
        "\n",
        "    policy_iteration_loops = 0\n",
        "    total_evaluation_sweeps = 0\n",
        "\n",
        "    while True:\n",
        "        policy_iteration_loops += 1\n",
        "        print(f\"\\n--- Policy Iteration Loop #{policy_iteration_loops} ---\")\n",
        "\n",
        "        # 2. Policy Evaluation\n",
        "        # 2. Policy Evaluation\n",
        "        print(\"Running Policy Evaluation...\")\n",
        "        eval_sweeps = 0\n",
        "        while True:\n",
        "            delta = 0\n",
        "            eval_sweeps += 1\n",
        "\n",
        "            # --- ADD THIS LINE ---\n",
        "            if eval_sweeps % 10 == 0:\n",
        "                print(f\"    ... PE sweep #{eval_sweeps}, current delta: {delta:.2e}\")\n",
        "            # --- END ADD ---\n",
        "\n",
        "            for s_idx in range(n_states):\n",
        "                v_old = V[s_idx]\n",
        "                v_new = 0\n",
        "\n",
        "                # Get the action from the current policy\n",
        "                a = policy[s_idx]\n",
        "\n",
        "                # Calculate expected value\n",
        "                for (prob, next_s_idx, reward) in model[s_idx][a]:\n",
        "                    v_new += prob * (reward + gamma * V[next_s_idx])\n",
        "\n",
        "                V[s_idx] = v_new\n",
        "                delta = max(delta, abs(v_old - v_new))\n",
        "\n",
        "            if delta < theta:\n",
        "                print(f\"Policy Evaluation converged in {eval_sweeps} sweeps.\")\n",
        "                total_evaluation_sweeps += eval_sweeps\n",
        "                break\n",
        "\n",
        "        # 3. Policy Improvement\n",
        "        print(\"Running Policy Improvement...\")\n",
        "        policy_stable = True\n",
        "        for s_idx in range(n_states):\n",
        "            old_action = policy[s_idx]\n",
        "\n",
        "            # Find the best action\n",
        "            action_values = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                for (prob, next_s_idx, reward) in model[s_idx][a]:\n",
        "                    action_values[a] += prob * (reward + gamma * V[next_s_idx])\n",
        "\n",
        "            policy[s_idx] = np.argmax(action_values)\n",
        "\n",
        "            if old_action != policy[s_idx]:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            print(f\"\\nPolicy converged and is stable after {policy_iteration_loops} iterations.\")\n",
        "            break\n",
        "\n",
        "    return policy, V, policy_iteration_loops, total_evaluation_sweeps"
      ],
      "metadata": {
        "id": "fM5cnPZDqkQB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Setup ---\n",
        "start_time = time.time()\n",
        "mapper = StateMapper(size=5)\n",
        "\n",
        "# --- 2. Build Model ---\n",
        "model = build_transition_model(mapper)\n",
        "model_build_time = time.time() - start_time\n",
        "print(f\"Model build took {model_build_time:.2f} seconds.\")\n",
        "\n",
        "# --- 3. Run Policy Iteration ---\n",
        "train_start_time = time.time()\n",
        "policy_table, value_table, pi_loops, pe_loops = policy_iteration(\n",
        "    model, mapper, gamma=0.99\n",
        ")\n",
        "train_time = time.time() - train_start_time\n",
        "print(f\"Policy Iteration took {train_time:.2f} seconds.\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n--- Total process complete in {total_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f1a2c00a88034fc68d9ccb8a2f6acb33",
            "11a70b73c0e141aba83e47c4b296e707",
            "92ca74a82cd143168c463511dc77e0ed",
            "fadb9bb15ab34a27bc91d4214b925569",
            "a9fc2ae9be0144c6b4a2648a9d96c850",
            "e717d73777934a0b917888828eeea39d",
            "ad8814820c6b4839859f703a06a667dc",
            "8d8677ee925b4d5a973226e12e224797",
            "6291b5ea04d14968afef816cac8e32c1",
            "67e2bb99e5a546b6a62f5b85c631c513",
            "20812befa52e4038a669c58e9fbe16fd"
          ]
        },
        "id": "ORTi0rEuqoOh",
        "outputId": "a70f04c4-8dff-45fb-8225-d9328612fa6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building transition model for 203125 states...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/203125 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a2c00a88034fc68d9ccb8a2f6acb33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model build complete.\n",
            "Model build took 60.21 seconds.\n",
            "\n",
            "--- Policy Iteration Loop #1 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "    ... PE sweep #130, current delta: 0.00e+00\n",
            "    ... PE sweep #140, current delta: 0.00e+00\n",
            "    ... PE sweep #150, current delta: 0.00e+00\n",
            "    ... PE sweep #160, current delta: 0.00e+00\n",
            "    ... PE sweep #170, current delta: 0.00e+00\n",
            "    ... PE sweep #180, current delta: 0.00e+00\n",
            "    ... PE sweep #190, current delta: 0.00e+00\n",
            "    ... PE sweep #200, current delta: 0.00e+00\n",
            "    ... PE sweep #210, current delta: 0.00e+00\n",
            "    ... PE sweep #220, current delta: 0.00e+00\n",
            "    ... PE sweep #230, current delta: 0.00e+00\n",
            "    ... PE sweep #240, current delta: 0.00e+00\n",
            "    ... PE sweep #250, current delta: 0.00e+00\n",
            "    ... PE sweep #260, current delta: 0.00e+00\n",
            "    ... PE sweep #270, current delta: 0.00e+00\n",
            "    ... PE sweep #280, current delta: 0.00e+00\n",
            "    ... PE sweep #290, current delta: 0.00e+00\n",
            "    ... PE sweep #300, current delta: 0.00e+00\n",
            "    ... PE sweep #310, current delta: 0.00e+00\n",
            "    ... PE sweep #320, current delta: 0.00e+00\n",
            "    ... PE sweep #330, current delta: 0.00e+00\n",
            "    ... PE sweep #340, current delta: 0.00e+00\n",
            "    ... PE sweep #350, current delta: 0.00e+00\n",
            "    ... PE sweep #360, current delta: 0.00e+00\n",
            "    ... PE sweep #370, current delta: 0.00e+00\n",
            "    ... PE sweep #380, current delta: 0.00e+00\n",
            "    ... PE sweep #390, current delta: 0.00e+00\n",
            "    ... PE sweep #400, current delta: 0.00e+00\n",
            "    ... PE sweep #410, current delta: 0.00e+00\n",
            "    ... PE sweep #420, current delta: 0.00e+00\n",
            "    ... PE sweep #430, current delta: 0.00e+00\n",
            "    ... PE sweep #440, current delta: 0.00e+00\n",
            "    ... PE sweep #450, current delta: 0.00e+00\n",
            "    ... PE sweep #460, current delta: 0.00e+00\n",
            "    ... PE sweep #470, current delta: 0.00e+00\n",
            "    ... PE sweep #480, current delta: 0.00e+00\n",
            "    ... PE sweep #490, current delta: 0.00e+00\n",
            "    ... PE sweep #500, current delta: 0.00e+00\n",
            "    ... PE sweep #510, current delta: 0.00e+00\n",
            "    ... PE sweep #520, current delta: 0.00e+00\n",
            "    ... PE sweep #530, current delta: 0.00e+00\n",
            "    ... PE sweep #540, current delta: 0.00e+00\n",
            "    ... PE sweep #550, current delta: 0.00e+00\n",
            "    ... PE sweep #560, current delta: 0.00e+00\n",
            "    ... PE sweep #570, current delta: 0.00e+00\n",
            "    ... PE sweep #580, current delta: 0.00e+00\n",
            "    ... PE sweep #590, current delta: 0.00e+00\n",
            "    ... PE sweep #600, current delta: 0.00e+00\n",
            "    ... PE sweep #610, current delta: 0.00e+00\n",
            "    ... PE sweep #620, current delta: 0.00e+00\n",
            "    ... PE sweep #630, current delta: 0.00e+00\n",
            "    ... PE sweep #640, current delta: 0.00e+00\n",
            "    ... PE sweep #650, current delta: 0.00e+00\n",
            "    ... PE sweep #660, current delta: 0.00e+00\n",
            "    ... PE sweep #670, current delta: 0.00e+00\n",
            "    ... PE sweep #680, current delta: 0.00e+00\n",
            "    ... PE sweep #690, current delta: 0.00e+00\n",
            "    ... PE sweep #700, current delta: 0.00e+00\n",
            "    ... PE sweep #710, current delta: 0.00e+00\n",
            "    ... PE sweep #720, current delta: 0.00e+00\n",
            "    ... PE sweep #730, current delta: 0.00e+00\n",
            "    ... PE sweep #740, current delta: 0.00e+00\n",
            "    ... PE sweep #750, current delta: 0.00e+00\n",
            "    ... PE sweep #760, current delta: 0.00e+00\n",
            "    ... PE sweep #770, current delta: 0.00e+00\n",
            "    ... PE sweep #780, current delta: 0.00e+00\n",
            "    ... PE sweep #790, current delta: 0.00e+00\n",
            "    ... PE sweep #800, current delta: 0.00e+00\n",
            "    ... PE sweep #810, current delta: 0.00e+00\n",
            "    ... PE sweep #820, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 821 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #2 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "    ... PE sweep #130, current delta: 0.00e+00\n",
            "    ... PE sweep #140, current delta: 0.00e+00\n",
            "    ... PE sweep #150, current delta: 0.00e+00\n",
            "    ... PE sweep #160, current delta: 0.00e+00\n",
            "    ... PE sweep #170, current delta: 0.00e+00\n",
            "    ... PE sweep #180, current delta: 0.00e+00\n",
            "    ... PE sweep #190, current delta: 0.00e+00\n",
            "    ... PE sweep #200, current delta: 0.00e+00\n",
            "    ... PE sweep #210, current delta: 0.00e+00\n",
            "    ... PE sweep #220, current delta: 0.00e+00\n",
            "    ... PE sweep #230, current delta: 0.00e+00\n",
            "    ... PE sweep #240, current delta: 0.00e+00\n",
            "    ... PE sweep #250, current delta: 0.00e+00\n",
            "    ... PE sweep #260, current delta: 0.00e+00\n",
            "    ... PE sweep #270, current delta: 0.00e+00\n",
            "    ... PE sweep #280, current delta: 0.00e+00\n",
            "    ... PE sweep #290, current delta: 0.00e+00\n",
            "    ... PE sweep #300, current delta: 0.00e+00\n",
            "    ... PE sweep #310, current delta: 0.00e+00\n",
            "    ... PE sweep #320, current delta: 0.00e+00\n",
            "    ... PE sweep #330, current delta: 0.00e+00\n",
            "    ... PE sweep #340, current delta: 0.00e+00\n",
            "    ... PE sweep #350, current delta: 0.00e+00\n",
            "    ... PE sweep #360, current delta: 0.00e+00\n",
            "    ... PE sweep #370, current delta: 0.00e+00\n",
            "    ... PE sweep #380, current delta: 0.00e+00\n",
            "    ... PE sweep #390, current delta: 0.00e+00\n",
            "    ... PE sweep #400, current delta: 0.00e+00\n",
            "    ... PE sweep #410, current delta: 0.00e+00\n",
            "    ... PE sweep #420, current delta: 0.00e+00\n",
            "    ... PE sweep #430, current delta: 0.00e+00\n",
            "    ... PE sweep #440, current delta: 0.00e+00\n",
            "    ... PE sweep #450, current delta: 0.00e+00\n",
            "    ... PE sweep #460, current delta: 0.00e+00\n",
            "    ... PE sweep #470, current delta: 0.00e+00\n",
            "    ... PE sweep #480, current delta: 0.00e+00\n",
            "    ... PE sweep #490, current delta: 0.00e+00\n",
            "    ... PE sweep #500, current delta: 0.00e+00\n",
            "    ... PE sweep #510, current delta: 0.00e+00\n",
            "    ... PE sweep #520, current delta: 0.00e+00\n",
            "    ... PE sweep #530, current delta: 0.00e+00\n",
            "    ... PE sweep #540, current delta: 0.00e+00\n",
            "    ... PE sweep #550, current delta: 0.00e+00\n",
            "    ... PE sweep #560, current delta: 0.00e+00\n",
            "    ... PE sweep #570, current delta: 0.00e+00\n",
            "    ... PE sweep #580, current delta: 0.00e+00\n",
            "    ... PE sweep #590, current delta: 0.00e+00\n",
            "    ... PE sweep #600, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 608 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #3 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "    ... PE sweep #130, current delta: 0.00e+00\n",
            "    ... PE sweep #140, current delta: 0.00e+00\n",
            "    ... PE sweep #150, current delta: 0.00e+00\n",
            "    ... PE sweep #160, current delta: 0.00e+00\n",
            "    ... PE sweep #170, current delta: 0.00e+00\n",
            "    ... PE sweep #180, current delta: 0.00e+00\n",
            "    ... PE sweep #190, current delta: 0.00e+00\n",
            "    ... PE sweep #200, current delta: 0.00e+00\n",
            "    ... PE sweep #210, current delta: 0.00e+00\n",
            "    ... PE sweep #220, current delta: 0.00e+00\n",
            "    ... PE sweep #230, current delta: 0.00e+00\n",
            "    ... PE sweep #240, current delta: 0.00e+00\n",
            "    ... PE sweep #250, current delta: 0.00e+00\n",
            "    ... PE sweep #260, current delta: 0.00e+00\n",
            "    ... PE sweep #270, current delta: 0.00e+00\n",
            "    ... PE sweep #280, current delta: 0.00e+00\n",
            "    ... PE sweep #290, current delta: 0.00e+00\n",
            "    ... PE sweep #300, current delta: 0.00e+00\n",
            "    ... PE sweep #310, current delta: 0.00e+00\n",
            "    ... PE sweep #320, current delta: 0.00e+00\n",
            "    ... PE sweep #330, current delta: 0.00e+00\n",
            "    ... PE sweep #340, current delta: 0.00e+00\n",
            "    ... PE sweep #350, current delta: 0.00e+00\n",
            "    ... PE sweep #360, current delta: 0.00e+00\n",
            "    ... PE sweep #370, current delta: 0.00e+00\n",
            "    ... PE sweep #380, current delta: 0.00e+00\n",
            "    ... PE sweep #390, current delta: 0.00e+00\n",
            "    ... PE sweep #400, current delta: 0.00e+00\n",
            "    ... PE sweep #410, current delta: 0.00e+00\n",
            "    ... PE sweep #420, current delta: 0.00e+00\n",
            "    ... PE sweep #430, current delta: 0.00e+00\n",
            "    ... PE sweep #440, current delta: 0.00e+00\n",
            "    ... PE sweep #450, current delta: 0.00e+00\n",
            "    ... PE sweep #460, current delta: 0.00e+00\n",
            "    ... PE sweep #470, current delta: 0.00e+00\n",
            "    ... PE sweep #480, current delta: 0.00e+00\n",
            "    ... PE sweep #490, current delta: 0.00e+00\n",
            "    ... PE sweep #500, current delta: 0.00e+00\n",
            "    ... PE sweep #510, current delta: 0.00e+00\n",
            "    ... PE sweep #520, current delta: 0.00e+00\n",
            "    ... PE sweep #530, current delta: 0.00e+00\n",
            "    ... PE sweep #540, current delta: 0.00e+00\n",
            "    ... PE sweep #550, current delta: 0.00e+00\n",
            "    ... PE sweep #560, current delta: 0.00e+00\n",
            "    ... PE sweep #570, current delta: 0.00e+00\n",
            "    ... PE sweep #580, current delta: 0.00e+00\n",
            "    ... PE sweep #590, current delta: 0.00e+00\n",
            "    ... PE sweep #600, current delta: 0.00e+00\n",
            "    ... PE sweep #610, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 615 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #4 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "    ... PE sweep #130, current delta: 0.00e+00\n",
            "    ... PE sweep #140, current delta: 0.00e+00\n",
            "    ... PE sweep #150, current delta: 0.00e+00\n",
            "    ... PE sweep #160, current delta: 0.00e+00\n",
            "    ... PE sweep #170, current delta: 0.00e+00\n",
            "    ... PE sweep #180, current delta: 0.00e+00\n",
            "    ... PE sweep #190, current delta: 0.00e+00\n",
            "    ... PE sweep #200, current delta: 0.00e+00\n",
            "    ... PE sweep #210, current delta: 0.00e+00\n",
            "    ... PE sweep #220, current delta: 0.00e+00\n",
            "    ... PE sweep #230, current delta: 0.00e+00\n",
            "    ... PE sweep #240, current delta: 0.00e+00\n",
            "    ... PE sweep #250, current delta: 0.00e+00\n",
            "    ... PE sweep #260, current delta: 0.00e+00\n",
            "    ... PE sweep #270, current delta: 0.00e+00\n",
            "    ... PE sweep #280, current delta: 0.00e+00\n",
            "    ... PE sweep #290, current delta: 0.00e+00\n",
            "    ... PE sweep #300, current delta: 0.00e+00\n",
            "    ... PE sweep #310, current delta: 0.00e+00\n",
            "    ... PE sweep #320, current delta: 0.00e+00\n",
            "    ... PE sweep #330, current delta: 0.00e+00\n",
            "    ... PE sweep #340, current delta: 0.00e+00\n",
            "    ... PE sweep #350, current delta: 0.00e+00\n",
            "    ... PE sweep #360, current delta: 0.00e+00\n",
            "    ... PE sweep #370, current delta: 0.00e+00\n",
            "    ... PE sweep #380, current delta: 0.00e+00\n",
            "    ... PE sweep #390, current delta: 0.00e+00\n",
            "    ... PE sweep #400, current delta: 0.00e+00\n",
            "    ... PE sweep #410, current delta: 0.00e+00\n",
            "    ... PE sweep #420, current delta: 0.00e+00\n",
            "    ... PE sweep #430, current delta: 0.00e+00\n",
            "    ... PE sweep #440, current delta: 0.00e+00\n",
            "    ... PE sweep #450, current delta: 0.00e+00\n",
            "    ... PE sweep #460, current delta: 0.00e+00\n",
            "    ... PE sweep #470, current delta: 0.00e+00\n",
            "    ... PE sweep #480, current delta: 0.00e+00\n",
            "    ... PE sweep #490, current delta: 0.00e+00\n",
            "    ... PE sweep #500, current delta: 0.00e+00\n",
            "    ... PE sweep #510, current delta: 0.00e+00\n",
            "    ... PE sweep #520, current delta: 0.00e+00\n",
            "    ... PE sweep #530, current delta: 0.00e+00\n",
            "    ... PE sweep #540, current delta: 0.00e+00\n",
            "    ... PE sweep #550, current delta: 0.00e+00\n",
            "    ... PE sweep #560, current delta: 0.00e+00\n",
            "    ... PE sweep #570, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 572 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #5 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "    ... PE sweep #130, current delta: 0.00e+00\n",
            "    ... PE sweep #140, current delta: 0.00e+00\n",
            "    ... PE sweep #150, current delta: 0.00e+00\n",
            "    ... PE sweep #160, current delta: 0.00e+00\n",
            "    ... PE sweep #170, current delta: 0.00e+00\n",
            "    ... PE sweep #180, current delta: 0.00e+00\n",
            "    ... PE sweep #190, current delta: 0.00e+00\n",
            "    ... PE sweep #200, current delta: 0.00e+00\n",
            "    ... PE sweep #210, current delta: 0.00e+00\n",
            "    ... PE sweep #220, current delta: 0.00e+00\n",
            "    ... PE sweep #230, current delta: 0.00e+00\n",
            "    ... PE sweep #240, current delta: 0.00e+00\n",
            "    ... PE sweep #250, current delta: 0.00e+00\n",
            "    ... PE sweep #260, current delta: 0.00e+00\n",
            "    ... PE sweep #270, current delta: 0.00e+00\n",
            "    ... PE sweep #280, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 289 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #6 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "    ... PE sweep #100, current delta: 0.00e+00\n",
            "    ... PE sweep #110, current delta: 0.00e+00\n",
            "    ... PE sweep #120, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 127 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #7 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "    ... PE sweep #80, current delta: 0.00e+00\n",
            "    ... PE sweep #90, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 90 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #8 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 65 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #9 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "    ... PE sweep #40, current delta: 0.00e+00\n",
            "    ... PE sweep #50, current delta: 0.00e+00\n",
            "    ... PE sweep #60, current delta: 0.00e+00\n",
            "    ... PE sweep #70, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 78 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "--- Policy Iteration Loop #10 ---\n",
            "Running Policy Evaluation...\n",
            "    ... PE sweep #10, current delta: 0.00e+00\n",
            "    ... PE sweep #20, current delta: 0.00e+00\n",
            "    ... PE sweep #30, current delta: 0.00e+00\n",
            "Policy Evaluation converged in 32 sweeps.\n",
            "Running Policy Improvement...\n",
            "\n",
            "Policy converged and is stable after 10 iterations.\n",
            "Policy Iteration took 1769.29 seconds.\n",
            "\n",
            "--- Total process complete in 1829.49 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing the trained policy and recording video...\")\n",
        "\n",
        "# Create a new env, wrapped for video recording\n",
        "env = GridMazeEnv(size=5, render_mode=\"rgb_array\")\n",
        "video_env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=\"videos\",\n",
        "    name_prefix=\"policy-iteration-agent\",\n",
        "    episode_trigger=lambda e: True  # Record every episode\n",
        ")\n",
        "\n",
        "# Run one full episode\n",
        "(obs, info) = video_env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "total_reward = 0\n",
        "steps = 0\n",
        "\n",
        "while not (terminated or truncated):\n",
        "    # 1. Get the state index from the observation\n",
        "    s_idx = mapper.state_to_index(obs)\n",
        "\n",
        "    # 2. Look up the optimal action from our trained policy\n",
        "    action = policy_table[s_idx]\n",
        "\n",
        "    # 3. Take the action\n",
        "    obs, reward, terminated, truncated, info = video_env.step(action)\n",
        "\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "video_env.close()\n",
        "env.close()\n",
        "\n",
        "print(f\"\\nEpisode finished in {steps} steps.\")\n",
        "print(f\"Total reward: {total_reward:.2f}\")\n",
        "print(\"Video saved to 'videos' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4shiZmYEqqUS",
        "outputId": "373236d5-5a8c-49e2-a691-ee3f6a4aab4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the trained policy and recording video...\n",
            "\n",
            "Episode finished in 11 steps.\n",
            "Total reward: 0.90\n",
            "Video saved to 'videos' folder.\n"
          ]
        }
      ]
    }
  ]
}